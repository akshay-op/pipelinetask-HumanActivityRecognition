# -*- coding: utf-8 -*-
"""kineticsTensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SXT5lp24Zi_bEH8f-oh57pW19y5qDut2
"""
#install necessary modules 


# TensorFlow and TF-Hub modules.
#all necessary modules 

import tensorflow as tf
import tensorflow_hub as hub #since we are taking pre trained model from tensorflow hub
import os
import cv2
import numpy as np
from urllib import request  # requires python3


"""
download the test dataset : 20 video files from the link

https://github.com/akshay-op/pipelinetask-testdataset

"""

# path were the 20 video files for finding inference aka classify is stored
folderpath=os.path.join('/content/drive/MyDrive/kinetics/videos/')
#change the folderpath to your folder path


# function for taking video from the stored path and applying necessary transformations to feed to the model

def load_video(path):
  cap = cv2.VideoCapture(path)
  frames = []
  try:
    while True:
      ret, frame = cap.read()
      if not ret:
        break
      
      frame = cv2.resize(frame, (224,224))#224x224 is size for input model
      frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
      frames.append(frame)
      if len(frames) ==0:
        break
  finally:
    cap.release()
  return np.array(frames) / 255.0 #normalizing



#labels of the kinetic dataset (400 classes)
KINETICS_URL = "https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt"
with request.urlopen(KINETICS_URL) as obj:
  labels = [line.decode("utf-8").strip() for line in obj.readlines()]
print("Found %d labels." % len(labels))



#loading pretrained Inflated 3D Convnet model trained for action recognition on Kinetics-400. from tensorflow hub.
i3d = hub.load("https://tfhub.dev/deepmind/i3d-kinetics-400/1").signatures['default']


#function for predicting the classes of the video file.
def predict(Videofile):
  # Add a batch axis to the to the sample video.
  model_input = tf.constant(Videofile, dtype=tf.float32)[tf.newaxis, ...]
  pred = i3d(model_input)['default'][0]
  print(" actions detected:")
  for i in np.argsort(pred)[-1:]:
    print(f"  {labels[i]}")

#predicting and displays the videos in folder.
for i in (os.listdir(folderpath)):
  videofile=load_video(os.path.join(folderpath,i))
  print("video file :",i)
  predict(videofile)
  print("----------------------------------------")

